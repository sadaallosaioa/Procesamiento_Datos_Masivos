{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1FcPlPL_lGaWrk0vQ1jeKZ9l9eJglp6Ct","timestamp":1705172691005}],"authorship_tag":"ABX9TyO/Aj+c8KL26XFjx/mjpoMq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"FUwlYH41lYwg"},"outputs":[],"source":["#Instalamos Java\n","!apt-get install openjdk-8-jdk-headless --quiet > /dev/null"]},{"cell_type":"code","source":["#.Descargamos Spark: para ello tenemos que indicar:\n","!wget --quiet https://dlcdn.apache.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3.tgz"],"metadata":{"id":"mrZ7i-Q2lpXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#descomprima Hadoop:\n","!tar xf spark-3.4.2-bin-hadoop3.tgz"],"metadata":{"id":"rFKLAcXclzdW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Creamos las variables de entorno de java y Spark. Para ello creamos una nueva nota con lo siguiente:\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.2-bin-hadoop3\""],"metadata":{"id":"UEGrcAiemjp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Instalamos findspark para que nos permita importar pyspark de forma sencilla como una librerÃ­a:\n","!pip install --quiet findspark"],"metadata":{"id":"I0hQWhzSm0uR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Iniciamos findspark:\n","import findspark\n","findspark.init()"],"metadata":{"id":"udr3PDKInD1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Ejecutamos el programa:\n","!$SPARK_HOME/bin/spark-submit /content/A2Ej2.py /content/0426 /content/salida"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MbX-U0BpSYL","executionInfo":{"status":"ok","timestamp":1705232905338,"user_tz":-60,"elapsed":18476,"user":{"displayName":"Saioa Sada Allo","userId":"15281570099881323413"}},"outputId":"a5795462-1a7a-4fea-8a08-96c8382f1e50"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["24/01/14 11:48:10 INFO SparkContext: Running Spark version 3.4.2\n","24/01/14 11:48:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","24/01/14 11:48:11 INFO ResourceUtils: ==============================================================\n","24/01/14 11:48:11 INFO ResourceUtils: No custom resources configured for spark.driver.\n","24/01/14 11:48:11 INFO ResourceUtils: ==============================================================\n","24/01/14 11:48:11 INFO SparkContext: Submitted application: CategoriaDeVideosMenosVista\n","24/01/14 11:48:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n","24/01/14 11:48:11 INFO ResourceProfile: Limiting resource is cpu\n","24/01/14 11:48:11 INFO ResourceProfileManager: Added ResourceProfile id: 0\n","24/01/14 11:48:11 INFO SecurityManager: Changing view acls to: root\n","24/01/14 11:48:11 INFO SecurityManager: Changing modify acls to: root\n","24/01/14 11:48:11 INFO SecurityManager: Changing view acls groups to: \n","24/01/14 11:48:11 INFO SecurityManager: Changing modify acls groups to: \n","24/01/14 11:48:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n","24/01/14 11:48:11 INFO Utils: Successfully started service 'sparkDriver' on port 40407.\n","24/01/14 11:48:11 INFO SparkEnv: Registering MapOutputTracker\n","24/01/14 11:48:11 INFO SparkEnv: Registering BlockManagerMaster\n","24/01/14 11:48:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n","24/01/14 11:48:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n","24/01/14 11:48:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/01/14 11:48:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9539490e-83a2-4d8f-b272-f3daa52e10f2\n","24/01/14 11:48:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n","24/01/14 11:48:12 INFO SparkEnv: Registering OutputCommitCoordinator\n","24/01/14 11:48:12 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n","24/01/14 11:48:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n","24/01/14 11:48:12 INFO Executor: Starting executor ID driver on host 8860c3bfd92a\n","24/01/14 11:48:12 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n","24/01/14 11:48:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36709.\n","24/01/14 11:48:13 INFO NettyBlockTransferService: Server created on 8860c3bfd92a:36709\n","24/01/14 11:48:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n","24/01/14 11:48:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8860c3bfd92a, 36709, None)\n","24/01/14 11:48:13 INFO BlockManagerMasterEndpoint: Registering block manager 8860c3bfd92a:36709 with 366.3 MiB RAM, BlockManagerId(driver, 8860c3bfd92a, 36709, None)\n","24/01/14 11:48:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8860c3bfd92a, 36709, None)\n","24/01/14 11:48:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8860c3bfd92a, 36709, None)\n","24/01/14 11:48:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 342.9 KiB, free 366.0 MiB)\n","24/01/14 11:48:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)\n","24/01/14 11:48:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8860c3bfd92a:36709 (size: 32.5 KiB, free: 366.3 MiB)\n","24/01/14 11:48:15 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0\n","24/01/14 11:48:16 INFO FileInputFormat: Total input files to process : 5\n","24/01/14 11:48:16 INFO FileInputFormat: Total input files to process : 5\n","24/01/14 11:48:16 INFO SparkContext: Starting job: sortBy at /content/A2Ej2.py:32\n","24/01/14 11:48:16 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/A2Ej2.py:32) as input to shuffle 0\n","24/01/14 11:48:17 INFO DAGScheduler: Got job 0 (sortBy at /content/A2Ej2.py:32) with 2 output partitions\n","24/01/14 11:48:17 INFO DAGScheduler: Final stage: ResultStage 1 (sortBy at /content/A2Ej2.py:32)\n","24/01/14 11:48:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n","24/01/14 11:48:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n","24/01/14 11:48:17 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/A2Ej2.py:32), which has no missing parents\n","24/01/14 11:48:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.3 KiB, free 365.9 MiB)\n","24/01/14 11:48:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 365.9 MiB)\n","24/01/14 11:48:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8860c3bfd92a:36709 (size: 7.8 KiB, free: 366.3 MiB)\n","24/01/14 11:48:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:17 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/A2Ej2.py:32) (first 15 tasks are for partitions Vector(0, 1))\n","24/01/14 11:48:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n","24/01/14 11:48:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8860c3bfd92a, executor driver, partition 0, PROCESS_LOCAL, 7509 bytes) \n","24/01/14 11:48:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (8860c3bfd92a, executor driver, partition 1, PROCESS_LOCAL, 7555 bytes) \n","24/01/14 11:48:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n","24/01/14 11:48:17 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n","24/01/14 11:48:18 INFO WholeTextFileRDD: Input split: Paths:/content/0426/2.txt:0+2243234,/content/0426/3.txt:0+15439146\n","24/01/14 11:48:18 INFO WholeTextFileRDD: Input split: Paths:/content/0426/1.txt:0+429421,/content/0426/0.txt:0+65063,/content/0426/log.txt:0+160\n","24/01/14 11:48:20 INFO PythonRunner: Times: total = 1338, boot = 841, init = 479, finish = 18\n","24/01/14 11:48:20 INFO PythonRunner: Times: total = 2034, boot = 825, init = 428, finish = 781\n","24/01/14 11:48:20 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1624 bytes result sent to driver\n","24/01/14 11:48:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1667 bytes result sent to driver\n","24/01/14 11:48:20 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3092 ms on 8860c3bfd92a (executor driver) (1/2)\n","24/01/14 11:48:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3154 ms on 8860c3bfd92a (executor driver) (2/2)\n","24/01/14 11:48:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n","24/01/14 11:48:20 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 60861\n","24/01/14 11:48:20 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/A2Ej2.py:32) finished in 3.543 s\n","24/01/14 11:48:20 INFO DAGScheduler: looking for newly runnable stages\n","24/01/14 11:48:20 INFO DAGScheduler: running: Set()\n","24/01/14 11:48:20 INFO DAGScheduler: waiting: Set(ResultStage 1)\n","24/01/14 11:48:20 INFO DAGScheduler: failed: Set()\n","24/01/14 11:48:20 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at sortBy at /content/A2Ej2.py:32), which has no missing parents\n","24/01/14 11:48:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.7 KiB, free 365.9 MiB)\n","24/01/14 11:48:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 365.9 MiB)\n","24/01/14 11:48:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 8860c3bfd92a:36709 (size: 6.6 KiB, free: 366.3 MiB)\n","24/01/14 11:48:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at sortBy at /content/A2Ej2.py:32) (first 15 tasks are for partitions Vector(0, 1))\n","24/01/14 11:48:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n","24/01/14 11:48:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (8860c3bfd92a, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n","24/01/14 11:48:20 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (8860c3bfd92a, executor driver, partition 1, NODE_LOCAL, 7181 bytes) \n","24/01/14 11:48:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n","24/01/14 11:48:20 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Getting 2 (284.0 B) non-empty blocks including 2 (284.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Getting 2 (527.0 B) non-empty blocks including 2 (527.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 36 ms\n","24/01/14 11:48:21 INFO PythonRunner: Times: total = 188, boot = -1147, init = 1335, finish = 0\n","24/01/14 11:48:21 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2054 bytes result sent to driver\n","24/01/14 11:48:21 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 335 ms on 8860c3bfd92a (executor driver) (1/2)\n","24/01/14 11:48:21 INFO PythonRunner: Times: total = 204, boot = -462, init = 666, finish = 0\n","24/01/14 11:48:21 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 2054 bytes result sent to driver\n","24/01/14 11:48:21 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 360 ms on 8860c3bfd92a (executor driver) (2/2)\n","24/01/14 11:48:21 INFO DAGScheduler: ResultStage 1 (sortBy at /content/A2Ej2.py:32) finished in 0.397 s\n","24/01/14 11:48:21 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n","24/01/14 11:48:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n","24/01/14 11:48:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n","24/01/14 11:48:21 INFO DAGScheduler: Job 0 finished: sortBy at /content/A2Ej2.py:32, took 4.482204 s\n","24/01/14 11:48:21 INFO SparkContext: Starting job: sortBy at /content/A2Ej2.py:32\n","24/01/14 11:48:21 INFO DAGScheduler: Got job 1 (sortBy at /content/A2Ej2.py:32) with 2 output partitions\n","24/01/14 11:48:21 INFO DAGScheduler: Final stage: ResultStage 3 (sortBy at /content/A2Ej2.py:32)\n","24/01/14 11:48:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n","24/01/14 11:48:21 INFO DAGScheduler: Missing parents: List()\n","24/01/14 11:48:21 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at sortBy at /content/A2Ej2.py:32), which has no missing parents\n","24/01/14 11:48:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.0 KiB, free 365.9 MiB)\n","24/01/14 11:48:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 365.9 MiB)\n","24/01/14 11:48:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 8860c3bfd92a:36709 (size: 6.4 KiB, free: 366.2 MiB)\n","24/01/14 11:48:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[7] at sortBy at /content/A2Ej2.py:32) (first 15 tasks are for partitions Vector(0, 1))\n","24/01/14 11:48:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n","24/01/14 11:48:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (8860c3bfd92a, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n","24/01/14 11:48:21 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5) (8860c3bfd92a, executor driver, partition 1, NODE_LOCAL, 7181 bytes) \n","24/01/14 11:48:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)\n","24/01/14 11:48:21 INFO Executor: Running task 1.0 in stage 3.0 (TID 5)\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Getting 2 (284.0 B) non-empty blocks including 2 (284.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Getting 2 (527.0 B) non-empty blocks including 2 (527.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n","24/01/14 11:48:21 INFO PythonRunner: Times: total = 204, boot = -149, init = 353, finish = 0\n","24/01/14 11:48:21 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 2093 bytes result sent to driver\n","24/01/14 11:48:21 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 262 ms on 8860c3bfd92a (executor driver) (1/2)\n","24/01/14 11:48:21 INFO PythonRunner: Times: total = 225, boot = -172, init = 397, finish = 0\n","24/01/14 11:48:21 INFO Executor: Finished task 1.0 in stage 3.0 (TID 5). 2175 bytes result sent to driver\n","24/01/14 11:48:21 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 275 ms on 8860c3bfd92a (executor driver) (2/2)\n","24/01/14 11:48:21 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n","24/01/14 11:48:21 INFO DAGScheduler: ResultStage 3 (sortBy at /content/A2Ej2.py:32) finished in 0.290 s\n","24/01/14 11:48:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n","24/01/14 11:48:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n","24/01/14 11:48:21 INFO DAGScheduler: Job 1 finished: sortBy at /content/A2Ej2.py:32, took 0.308832 s\n","24/01/14 11:48:21 INFO SparkContext: Starting job: runJob at PythonRDD.scala:179\n","24/01/14 11:48:21 INFO DAGScheduler: Registering RDD 9 (sortBy at /content/A2Ej2.py:32) as input to shuffle 1\n","24/01/14 11:48:21 INFO DAGScheduler: Got job 2 (runJob at PythonRDD.scala:179) with 1 output partitions\n","24/01/14 11:48:21 INFO DAGScheduler: Final stage: ResultStage 6 (runJob at PythonRDD.scala:179)\n","24/01/14 11:48:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n","24/01/14 11:48:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)\n","24/01/14 11:48:21 INFO DAGScheduler: Submitting ShuffleMapStage 5 (PairwiseRDD[9] at sortBy at /content/A2Ej2.py:32), which has no missing parents\n","24/01/14 11:48:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.8 KiB, free 365.9 MiB)\n","24/01/14 11:48:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.9 MiB)\n","24/01/14 11:48:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 8860c3bfd92a:36709 (size: 7.2 KiB, free: 366.2 MiB)\n","24/01/14 11:48:21 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:21 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (PairwiseRDD[9] at sortBy at /content/A2Ej2.py:32) (first 15 tasks are for partitions Vector(0, 1))\n","24/01/14 11:48:21 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n","24/01/14 11:48:21 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (8860c3bfd92a, executor driver, partition 0, NODE_LOCAL, 7170 bytes) \n","24/01/14 11:48:21 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (8860c3bfd92a, executor driver, partition 1, NODE_LOCAL, 7170 bytes) \n","24/01/14 11:48:21 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)\n","24/01/14 11:48:21 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Getting 2 (284.0 B) non-empty blocks including 2 (284.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Getting 2 (527.0 B) non-empty blocks including 2 (527.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n","24/01/14 11:48:22 INFO PythonRunner: Times: total = 217, boot = -102, init = 318, finish = 1\n","24/01/14 11:48:22 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2226 bytes result sent to driver\n","24/01/14 11:48:22 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 276 ms on 8860c3bfd92a (executor driver) (1/2)\n","24/01/14 11:48:22 INFO PythonRunner: Times: total = 263, boot = -113, init = 374, finish = 2\n","24/01/14 11:48:22 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2226 bytes result sent to driver\n","24/01/14 11:48:22 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 311 ms on 8860c3bfd92a (executor driver) (2/2)\n","24/01/14 11:48:22 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n","24/01/14 11:48:22 INFO DAGScheduler: ShuffleMapStage 5 (sortBy at /content/A2Ej2.py:32) finished in 0.333 s\n","24/01/14 11:48:22 INFO DAGScheduler: looking for newly runnable stages\n","24/01/14 11:48:22 INFO DAGScheduler: running: Set()\n","24/01/14 11:48:22 INFO DAGScheduler: waiting: Set(ResultStage 6)\n","24/01/14 11:48:22 INFO DAGScheduler: failed: Set()\n","24/01/14 11:48:22 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[12] at RDD at PythonRDD.scala:53), which has no missing parents\n","24/01/14 11:48:22 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.9 KiB, free 365.8 MiB)\n","24/01/14 11:48:22 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 365.8 MiB)\n","24/01/14 11:48:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 8860c3bfd92a:36709 (size: 5.9 KiB, free: 366.2 MiB)\n","24/01/14 11:48:22 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (PythonRDD[12] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n","24/01/14 11:48:22 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n","24/01/14 11:48:22 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (8860c3bfd92a, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n","24/01/14 11:48:22 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)\n","24/01/14 11:48:22 INFO ShuffleBlockFetcherIterator: Getting 2 (325.0 B) non-empty blocks including 2 (325.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n","24/01/14 11:48:22 INFO PythonRunner: Times: total = 100, boot = -51, init = 150, finish = 1\n","24/01/14 11:48:22 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 2077 bytes result sent to driver\n","24/01/14 11:48:22 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 134 ms on 8860c3bfd92a (executor driver) (1/1)\n","24/01/14 11:48:22 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n","24/01/14 11:48:22 INFO DAGScheduler: ResultStage 6 (runJob at PythonRDD.scala:179) finished in 0.153 s\n","24/01/14 11:48:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n","24/01/14 11:48:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n","24/01/14 11:48:22 INFO DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:179, took 0.512511 s\n","24/01/14 11:48:22 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n","24/01/14 11:48:22 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n","24/01/14 11:48:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","24/01/14 11:48:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","24/01/14 11:48:22 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n","24/01/14 11:48:22 INFO DAGScheduler: Got job 3 (runJob at SparkHadoopWriter.scala:83) with 2 output partitions\n","24/01/14 11:48:22 INFO DAGScheduler: Final stage: ResultStage 7 (runJob at SparkHadoopWriter.scala:83)\n","24/01/14 11:48:22 INFO DAGScheduler: Parents of final stage: List()\n","24/01/14 11:48:22 INFO DAGScheduler: Missing parents: List()\n","24/01/14 11:48:22 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[16] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n","24/01/14 11:48:22 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 104.2 KiB, free 365.7 MiB)\n","24/01/14 11:48:22 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 365.7 MiB)\n","24/01/14 11:48:22 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 8860c3bfd92a:36709 (size: 38.4 KiB, free: 366.2 MiB)\n","24/01/14 11:48:22 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[16] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n","24/01/14 11:48:22 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n","24/01/14 11:48:22 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9) (8860c3bfd92a, executor driver, partition 0, PROCESS_LOCAL, 7343 bytes) \n","24/01/14 11:48:22 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 10) (8860c3bfd92a, executor driver, partition 1, PROCESS_LOCAL, 7393 bytes) \n","24/01/14 11:48:22 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)\n","24/01/14 11:48:22 INFO Executor: Running task 1.0 in stage 7.0 (TID 10)\n","24/01/14 11:48:22 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n","24/01/14 11:48:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","24/01/14 11:48:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","24/01/14 11:48:22 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n","24/01/14 11:48:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","24/01/14 11:48:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","24/01/14 11:48:22 INFO PythonRunner: Times: total = 267, boot = -469, init = 736, finish = 0\n","24/01/14 11:48:22 INFO FileOutputCommitter: Saved output of task 'attempt_202401141148227885405865275576275_0016_m_000000_0' to file:/content/salida/_temporary/0/task_202401141148227885405865275576275_0016_m_000000\n","24/01/14 11:48:22 INFO SparkHadoopMapRedUtil: attempt_202401141148227885405865275576275_0016_m_000000_0: Committed. Elapsed time: 3 ms.\n","24/01/14 11:48:22 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1577 bytes result sent to driver\n","24/01/14 11:48:22 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 400 ms on 8860c3bfd92a (executor driver) (1/2)\n","24/01/14 11:48:22 INFO PythonRunner: Times: total = 321, boot = -315, init = 636, finish = 0\n","24/01/14 11:48:22 INFO FileOutputCommitter: Saved output of task 'attempt_202401141148227885405865275576275_0016_m_000001_0' to file:/content/salida/_temporary/0/task_202401141148227885405865275576275_0016_m_000001\n","24/01/14 11:48:22 INFO SparkHadoopMapRedUtil: attempt_202401141148227885405865275576275_0016_m_000001_0: Committed. Elapsed time: 12 ms.\n","24/01/14 11:48:22 INFO Executor: Finished task 1.0 in stage 7.0 (TID 10). 1620 bytes result sent to driver\n","24/01/14 11:48:22 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 10) in 442 ms on 8860c3bfd92a (executor driver) (2/2)\n","24/01/14 11:48:22 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n","24/01/14 11:48:22 INFO DAGScheduler: ResultStage 7 (runJob at SparkHadoopWriter.scala:83) finished in 0.482 s\n","24/01/14 11:48:22 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n","24/01/14 11:48:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n","24/01/14 11:48:22 INFO DAGScheduler: Job 3 finished: runJob at SparkHadoopWriter.scala:83, took 0.495205 s\n","24/01/14 11:48:22 INFO SparkHadoopWriter: Start to commit write Job job_202401141148227885405865275576275_0016.\n","24/01/14 11:48:22 INFO SparkHadoopWriter: Write Job job_202401141148227885405865275576275_0016 committed. Elapsed time: 17 ms.\n","24/01/14 11:48:22 INFO SparkContext: SparkContext is stopping with exitCode 0.\n","24/01/14 11:48:23 INFO SparkUI: Stopped Spark web UI at http://8860c3bfd92a:4040\n","24/01/14 11:48:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n","24/01/14 11:48:23 INFO MemoryStore: MemoryStore cleared\n","24/01/14 11:48:23 INFO BlockManager: BlockManager stopped\n","24/01/14 11:48:23 INFO BlockManagerMaster: BlockManagerMaster stopped\n","24/01/14 11:48:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n","24/01/14 11:48:23 INFO SparkContext: Successfully stopped SparkContext\n","24/01/14 11:48:24 INFO ShutdownHookManager: Shutdown hook called\n","24/01/14 11:48:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-7fae18ce-6b6d-48a2-afe4-ef2eb500d514/pyspark-dd8e3d41-3556-48d1-bde8-cc3cc8c99bed\n","24/01/14 11:48:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-ae77ec5e-9cb0-4e03-ac3a-5223ea66df09\n","24/01/14 11:48:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-7fae18ce-6b6d-48a2-afe4-ef2eb500d514\n"]}]},{"cell_type":"code","source":["!cat salida/*"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QlRfkRWGpcJh","executionInfo":{"status":"ok","timestamp":1705232969905,"user_tz":-60,"elapsed":489,"user":{"displayName":"Saioa Sada Allo","userId":"15281570099881323413"}},"outputId":"1b711e80-23df-41d1-b757-6c3f8a29d895"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["('Travel & Places', 2658724)\n"]}]},{"cell_type":"code","source":["# Ahora con otra carpeta de archivos diferente:\n","!$SPARK_HOME/bin/spark-submit /content/A2Ej2.py /content/0301 /content/salida301"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zZdzg7GVzAGc","executionInfo":{"status":"ok","timestamp":1705232937049,"user_tz":-60,"elapsed":22756,"user":{"displayName":"Saioa Sada Allo","userId":"15281570099881323413"}},"outputId":"ceaa10ba-d5d4-4b4b-ab06-8c2c969966e3"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["24/01/14 11:48:40 INFO SparkContext: Running Spark version 3.4.2\n","24/01/14 11:48:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","24/01/14 11:48:41 INFO ResourceUtils: ==============================================================\n","24/01/14 11:48:41 INFO ResourceUtils: No custom resources configured for spark.driver.\n","24/01/14 11:48:41 INFO ResourceUtils: ==============================================================\n","24/01/14 11:48:41 INFO SparkContext: Submitted application: CategoriaDeVideosMenosVista\n","24/01/14 11:48:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n","24/01/14 11:48:41 INFO ResourceProfile: Limiting resource is cpu\n","24/01/14 11:48:41 INFO ResourceProfileManager: Added ResourceProfile id: 0\n","24/01/14 11:48:41 INFO SecurityManager: Changing view acls to: root\n","24/01/14 11:48:41 INFO SecurityManager: Changing modify acls to: root\n","24/01/14 11:48:41 INFO SecurityManager: Changing view acls groups to: \n","24/01/14 11:48:41 INFO SecurityManager: Changing modify acls groups to: \n","24/01/14 11:48:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n","24/01/14 11:48:42 INFO Utils: Successfully started service 'sparkDriver' on port 44465.\n","24/01/14 11:48:43 INFO SparkEnv: Registering MapOutputTracker\n","24/01/14 11:48:43 INFO SparkEnv: Registering BlockManagerMaster\n","24/01/14 11:48:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n","24/01/14 11:48:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n","24/01/14 11:48:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","24/01/14 11:48:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6b6c3fe4-6f59-4ad3-a226-6466dbe72bc5\n","24/01/14 11:48:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n","24/01/14 11:48:43 INFO SparkEnv: Registering OutputCommitCoordinator\n","24/01/14 11:48:44 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n","24/01/14 11:48:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n","24/01/14 11:48:45 INFO Executor: Starting executor ID driver on host 8860c3bfd92a\n","24/01/14 11:48:45 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n","24/01/14 11:48:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39727.\n","24/01/14 11:48:45 INFO NettyBlockTransferService: Server created on 8860c3bfd92a:39727\n","24/01/14 11:48:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n","24/01/14 11:48:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 8860c3bfd92a, 39727, None)\n","24/01/14 11:48:45 INFO BlockManagerMasterEndpoint: Registering block manager 8860c3bfd92a:39727 with 366.3 MiB RAM, BlockManagerId(driver, 8860c3bfd92a, 39727, None)\n","24/01/14 11:48:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 8860c3bfd92a, 39727, None)\n","24/01/14 11:48:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 8860c3bfd92a, 39727, None)\n","24/01/14 11:48:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 342.9 KiB, free 366.0 MiB)\n","24/01/14 11:48:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 365.9 MiB)\n","24/01/14 11:48:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 8860c3bfd92a:39727 (size: 32.5 KiB, free: 366.3 MiB)\n","24/01/14 11:48:47 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0\n","24/01/14 11:48:47 INFO FileInputFormat: Total input files to process : 5\n","24/01/14 11:48:47 INFO FileInputFormat: Total input files to process : 5\n","24/01/14 11:48:48 INFO SparkContext: Starting job: sortBy at /content/A2Ej2.py:32\n","24/01/14 11:48:48 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/A2Ej2.py:32) as input to shuffle 0\n","24/01/14 11:48:48 INFO DAGScheduler: Got job 0 (sortBy at /content/A2Ej2.py:32) with 2 output partitions\n","24/01/14 11:48:48 INFO DAGScheduler: Final stage: ResultStage 1 (sortBy at /content/A2Ej2.py:32)\n","24/01/14 11:48:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n","24/01/14 11:48:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n","24/01/14 11:48:48 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/A2Ej2.py:32), which has no missing parents\n","24/01/14 11:48:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.3 KiB, free 365.9 MiB)\n","24/01/14 11:48:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 365.9 MiB)\n","24/01/14 11:48:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 8860c3bfd92a:39727 (size: 7.8 KiB, free: 366.3 MiB)\n","24/01/14 11:48:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/A2Ej2.py:32) (first 15 tasks are for partitions Vector(0, 1))\n","24/01/14 11:48:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n","24/01/14 11:48:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (8860c3bfd92a, executor driver, partition 0, PROCESS_LOCAL, 7509 bytes) \n","24/01/14 11:48:48 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (8860c3bfd92a, executor driver, partition 1, PROCESS_LOCAL, 7555 bytes) \n","24/01/14 11:48:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n","24/01/14 11:48:48 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n","24/01/14 11:48:49 INFO WholeTextFileRDD: Input split: Paths:/content/0301/2.txt:0+5643244,/content/0301/3.txt:0+24117248\n","24/01/14 11:48:49 INFO WholeTextFileRDD: Input split: Paths:/content/0301/1.txt:0+1043164,/content/0301/0.txt:0+104897,/content/0301/log.txt:0+168\n","24/01/14 11:48:52 INFO PythonRunner: Times: total = 1627, boot = 1039, init = 504, finish = 84\n","24/01/14 11:48:52 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1667 bytes result sent to driver\n","24/01/14 11:48:52 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3365 ms on 8860c3bfd92a (executor driver) (1/2)\n","24/01/14 11:48:52 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 41235\n","24/01/14 11:48:52 INFO PythonRunner: Times: total = 2794, boot = 1048, init = 347, finish = 1399\n","24/01/14 11:48:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1667 bytes result sent to driver\n","24/01/14 11:48:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3586 ms on 8860c3bfd92a (executor driver) (2/2)\n","24/01/14 11:48:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n","24/01/14 11:48:52 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/A2Ej2.py:32) finished in 3.816 s\n","24/01/14 11:48:52 INFO DAGScheduler: looking for newly runnable stages\n","24/01/14 11:48:52 INFO DAGScheduler: running: Set()\n","24/01/14 11:48:52 INFO DAGScheduler: waiting: Set(ResultStage 1)\n","24/01/14 11:48:52 INFO DAGScheduler: failed: Set()\n","24/01/14 11:48:52 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at sortBy at /content/A2Ej2.py:32), which has no missing parents\n","24/01/14 11:48:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.7 KiB, free 365.9 MiB)\n","24/01/14 11:48:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 365.9 MiB)\n","24/01/14 11:48:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 8860c3bfd92a:39727 (size: 6.6 KiB, free: 366.3 MiB)\n","24/01/14 11:48:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at sortBy at /content/A2Ej2.py:32) (first 15 tasks are for partitions Vector(0, 1))\n","24/01/14 11:48:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n","24/01/14 11:48:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (8860c3bfd92a, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n","24/01/14 11:48:52 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (8860c3bfd92a, executor driver, partition 1, NODE_LOCAL, 7181 bytes) \n","24/01/14 11:48:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n","24/01/14 11:48:52 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n","24/01/14 11:48:52 INFO ShuffleBlockFetcherIterator: Getting 2 (527.0 B) non-empty blocks including 2 (527.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 22 ms\n","24/01/14 11:48:52 INFO ShuffleBlockFetcherIterator: Getting 2 (284.0 B) non-empty blocks including 2 (284.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms\n","24/01/14 11:48:52 INFO PythonRunner: Times: total = 219, boot = -1314, init = 1533, finish = 0\n","24/01/14 11:48:52 INFO PythonRunner: Times: total = 233, boot = -182, init = 414, finish = 1\n","24/01/14 11:48:52 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 2097 bytes result sent to driver\n","24/01/14 11:48:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 2097 bytes result sent to driver\n","24/01/14 11:48:52 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 383 ms on 8860c3bfd92a (executor driver) (1/2)\n","24/01/14 11:48:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 395 ms on 8860c3bfd92a (executor driver) (2/2)\n","24/01/14 11:48:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n","24/01/14 11:48:52 INFO DAGScheduler: ResultStage 1 (sortBy at /content/A2Ej2.py:32) finished in 0.426 s\n","24/01/14 11:48:52 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n","24/01/14 11:48:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n","24/01/14 11:48:52 INFO DAGScheduler: Job 0 finished: sortBy at /content/A2Ej2.py:32, took 4.689591 s\n","24/01/14 11:48:53 INFO SparkContext: Starting job: sortBy at /content/A2Ej2.py:32\n","24/01/14 11:48:53 INFO DAGScheduler: Got job 1 (sortBy at /content/A2Ej2.py:32) with 2 output partitions\n","24/01/14 11:48:53 INFO DAGScheduler: Final stage: ResultStage 3 (sortBy at /content/A2Ej2.py:32)\n","24/01/14 11:48:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n","24/01/14 11:48:53 INFO DAGScheduler: Missing parents: List()\n","24/01/14 11:48:53 INFO DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at sortBy at /content/A2Ej2.py:32), which has no missing parents\n","24/01/14 11:48:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.0 KiB, free 365.9 MiB)\n","24/01/14 11:48:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 365.9 MiB)\n","24/01/14 11:48:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 8860c3bfd92a:39727 (size: 6.4 KiB, free: 366.2 MiB)\n","24/01/14 11:48:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[7] at sortBy at /content/A2Ej2.py:32) (first 15 tasks are for partitions Vector(0, 1))\n","24/01/14 11:48:53 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n","24/01/14 11:48:53 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (8860c3bfd92a, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n","24/01/14 11:48:53 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5) (8860c3bfd92a, executor driver, partition 1, NODE_LOCAL, 7181 bytes) \n","24/01/14 11:48:53 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)\n","24/01/14 11:48:53 INFO Executor: Running task 1.0 in stage 3.0 (TID 5)\n","24/01/14 11:48:53 INFO ShuffleBlockFetcherIterator: Getting 2 (527.0 B) non-empty blocks including 2 (527.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n","24/01/14 11:48:53 INFO ShuffleBlockFetcherIterator: Getting 2 (284.0 B) non-empty blocks including 2 (284.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n","24/01/14 11:48:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 8860c3bfd92a:39727 in memory (size: 7.8 KiB, free: 366.3 MiB)\n","24/01/14 11:48:53 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 8860c3bfd92a:39727 in memory (size: 6.6 KiB, free: 366.3 MiB)\n","24/01/14 11:48:53 INFO PythonRunner: Times: total = 251, boot = -130, init = 381, finish = 0\n","24/01/14 11:48:53 INFO Executor: Finished task 1.0 in stage 3.0 (TID 5). 2218 bytes result sent to driver\n","24/01/14 11:48:53 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 302 ms on 8860c3bfd92a (executor driver) (1/2)\n","24/01/14 11:48:53 INFO PythonRunner: Times: total = 232, boot = -221, init = 453, finish = 0\n","24/01/14 11:48:53 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 2136 bytes result sent to driver\n","24/01/14 11:48:53 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 368 ms on 8860c3bfd92a (executor driver) (2/2)\n","24/01/14 11:48:53 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n","24/01/14 11:48:53 INFO DAGScheduler: ResultStage 3 (sortBy at /content/A2Ej2.py:32) finished in 0.401 s\n","24/01/14 11:48:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n","24/01/14 11:48:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n","24/01/14 11:48:53 INFO DAGScheduler: Job 1 finished: sortBy at /content/A2Ej2.py:32, took 0.417618 s\n","24/01/14 11:48:53 INFO SparkContext: Starting job: runJob at PythonRDD.scala:179\n","24/01/14 11:48:53 INFO DAGScheduler: Registering RDD 9 (sortBy at /content/A2Ej2.py:32) as input to shuffle 1\n","24/01/14 11:48:53 INFO DAGScheduler: Got job 2 (runJob at PythonRDD.scala:179) with 1 output partitions\n","24/01/14 11:48:53 INFO DAGScheduler: Final stage: ResultStage 6 (runJob at PythonRDD.scala:179)\n","24/01/14 11:48:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n","24/01/14 11:48:53 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)\n","24/01/14 11:48:53 INFO DAGScheduler: Submitting ShuffleMapStage 5 (PairwiseRDD[9] at sortBy at /content/A2Ej2.py:32), which has no missing parents\n","24/01/14 11:48:53 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.8 KiB, free 365.9 MiB)\n","24/01/14 11:48:53 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 365.9 MiB)\n","24/01/14 11:48:53 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 8860c3bfd92a:39727 (size: 7.2 KiB, free: 366.3 MiB)\n","24/01/14 11:48:53 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:53 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (PairwiseRDD[9] at sortBy at /content/A2Ej2.py:32) (first 15 tasks are for partitions Vector(0, 1))\n","24/01/14 11:48:53 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n","24/01/14 11:48:53 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (8860c3bfd92a, executor driver, partition 0, NODE_LOCAL, 7170 bytes) \n","24/01/14 11:48:53 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (8860c3bfd92a, executor driver, partition 1, NODE_LOCAL, 7170 bytes) \n","24/01/14 11:48:53 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)\n","24/01/14 11:48:53 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)\n","24/01/14 11:48:53 INFO ShuffleBlockFetcherIterator: Getting 2 (284.0 B) non-empty blocks including 2 (284.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:53 INFO ShuffleBlockFetcherIterator: Getting 2 (527.0 B) non-empty blocks including 2 (527.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n","24/01/14 11:48:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n","24/01/14 11:48:53 INFO PythonRunner: Times: total = 216, boot = -151, init = 367, finish = 0\n","24/01/14 11:48:53 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2226 bytes result sent to driver\n","24/01/14 11:48:53 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 278 ms on 8860c3bfd92a (executor driver) (1/2)\n","24/01/14 11:48:53 INFO PythonRunner: Times: total = 261, boot = -86, init = 346, finish = 1\n","24/01/14 11:48:53 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2226 bytes result sent to driver\n","24/01/14 11:48:53 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 315 ms on 8860c3bfd92a (executor driver) (2/2)\n","24/01/14 11:48:53 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n","24/01/14 11:48:53 INFO DAGScheduler: ShuffleMapStage 5 (sortBy at /content/A2Ej2.py:32) finished in 0.330 s\n","24/01/14 11:48:53 INFO DAGScheduler: looking for newly runnable stages\n","24/01/14 11:48:53 INFO DAGScheduler: running: Set()\n","24/01/14 11:48:53 INFO DAGScheduler: waiting: Set(ResultStage 6)\n","24/01/14 11:48:53 INFO DAGScheduler: failed: Set()\n","24/01/14 11:48:53 INFO DAGScheduler: Submitting ResultStage 6 (PythonRDD[12] at RDD at PythonRDD.scala:53), which has no missing parents\n","24/01/14 11:48:53 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.9 KiB, free 365.9 MiB)\n","24/01/14 11:48:53 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 365.9 MiB)\n","24/01/14 11:48:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 8860c3bfd92a:39727 (size: 5.9 KiB, free: 366.2 MiB)\n","24/01/14 11:48:53 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (PythonRDD[12] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n","24/01/14 11:48:53 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n","24/01/14 11:48:53 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (8860c3bfd92a, executor driver, partition 0, NODE_LOCAL, 7181 bytes) \n","24/01/14 11:48:53 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)\n","24/01/14 11:48:53 INFO ShuffleBlockFetcherIterator: Getting 2 (325.0 B) non-empty blocks including 2 (325.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n","24/01/14 11:48:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n","24/01/14 11:48:53 INFO PythonRunner: Times: total = 116, boot = -69, init = 185, finish = 0\n","24/01/14 11:48:53 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 2077 bytes result sent to driver\n","24/01/14 11:48:53 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 148 ms on 8860c3bfd92a (executor driver) (1/1)\n","24/01/14 11:48:53 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n","24/01/14 11:48:53 INFO DAGScheduler: ResultStage 6 (runJob at PythonRDD.scala:179) finished in 0.164 s\n","24/01/14 11:48:53 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n","24/01/14 11:48:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n","24/01/14 11:48:53 INFO DAGScheduler: Job 2 finished: runJob at PythonRDD.scala:179, took 0.508343 s\n","24/01/14 11:48:54 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n","24/01/14 11:48:54 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n","24/01/14 11:48:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","24/01/14 11:48:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","24/01/14 11:48:54 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n","24/01/14 11:48:54 INFO DAGScheduler: Got job 3 (runJob at SparkHadoopWriter.scala:83) with 2 output partitions\n","24/01/14 11:48:54 INFO DAGScheduler: Final stage: ResultStage 7 (runJob at SparkHadoopWriter.scala:83)\n","24/01/14 11:48:54 INFO DAGScheduler: Parents of final stage: List()\n","24/01/14 11:48:54 INFO DAGScheduler: Missing parents: List()\n","24/01/14 11:48:54 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[16] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n","24/01/14 11:48:54 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 104.2 KiB, free 365.8 MiB)\n","24/01/14 11:48:54 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 365.7 MiB)\n","24/01/14 11:48:54 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 8860c3bfd92a:39727 (size: 38.4 KiB, free: 366.2 MiB)\n","24/01/14 11:48:54 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n","24/01/14 11:48:54 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[16] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n","24/01/14 11:48:54 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0\n","24/01/14 11:48:54 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9) (8860c3bfd92a, executor driver, partition 0, PROCESS_LOCAL, 7343 bytes) \n","24/01/14 11:48:54 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 10) (8860c3bfd92a, executor driver, partition 1, PROCESS_LOCAL, 7393 bytes) \n","24/01/14 11:48:54 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)\n","24/01/14 11:48:54 INFO Executor: Running task 1.0 in stage 7.0 (TID 10)\n","24/01/14 11:48:54 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n","24/01/14 11:48:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","24/01/14 11:48:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","24/01/14 11:48:54 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n","24/01/14 11:48:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","24/01/14 11:48:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","24/01/14 11:48:54 INFO PythonRunner: Times: total = 257, boot = -340, init = 597, finish = 0\n","24/01/14 11:48:54 INFO FileOutputCommitter: Saved output of task 'attempt_202401141148542837265106333016650_0016_m_000000_0' to file:/content/salida301/_temporary/0/task_202401141148542837265106333016650_0016_m_000000\n","24/01/14 11:48:54 INFO SparkHadoopMapRedUtil: attempt_202401141148542837265106333016650_0016_m_000000_0: Committed. Elapsed time: 8 ms.\n","24/01/14 11:48:54 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1577 bytes result sent to driver\n","24/01/14 11:48:54 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 448 ms on 8860c3bfd92a (executor driver) (1/2)\n","24/01/14 11:48:54 INFO PythonRunner: Times: total = 346, boot = -515, init = 861, finish = 0\n","24/01/14 11:48:54 INFO FileOutputCommitter: Saved output of task 'attempt_202401141148542837265106333016650_0016_m_000001_0' to file:/content/salida301/_temporary/0/task_202401141148542837265106333016650_0016_m_000001\n","24/01/14 11:48:54 INFO SparkHadoopMapRedUtil: attempt_202401141148542837265106333016650_0016_m_000001_0: Committed. Elapsed time: 1 ms.\n","24/01/14 11:48:54 INFO Executor: Finished task 1.0 in stage 7.0 (TID 10). 1620 bytes result sent to driver\n","24/01/14 11:48:54 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 10) in 491 ms on 8860c3bfd92a (executor driver) (2/2)\n","24/01/14 11:48:54 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n","24/01/14 11:48:54 INFO DAGScheduler: ResultStage 7 (runJob at SparkHadoopWriter.scala:83) finished in 0.538 s\n","24/01/14 11:48:54 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n","24/01/14 11:48:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n","24/01/14 11:48:54 INFO DAGScheduler: Job 3 finished: runJob at SparkHadoopWriter.scala:83, took 0.546033 s\n","24/01/14 11:48:54 INFO SparkHadoopWriter: Start to commit write Job job_202401141148542837265106333016650_0016.\n","24/01/14 11:48:54 INFO SparkHadoopWriter: Write Job job_202401141148542837265106333016650_0016 committed. Elapsed time: 40 ms.\n","24/01/14 11:48:54 INFO SparkContext: SparkContext is stopping with exitCode 0.\n","24/01/14 11:48:54 INFO SparkUI: Stopped Spark web UI at http://8860c3bfd92a:4040\n","24/01/14 11:48:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n","24/01/14 11:48:54 INFO MemoryStore: MemoryStore cleared\n","24/01/14 11:48:54 INFO BlockManager: BlockManager stopped\n","24/01/14 11:48:54 INFO BlockManagerMaster: BlockManagerMaster stopped\n","24/01/14 11:48:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n","24/01/14 11:48:54 INFO SparkContext: Successfully stopped SparkContext\n","24/01/14 11:48:55 INFO ShutdownHookManager: Shutdown hook called\n","24/01/14 11:48:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-fd09615d-f203-4c34-809c-67575c9c850e\n","24/01/14 11:48:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-fd09615d-f203-4c34-809c-67575c9c850e/pyspark-e150f486-9bde-47f4-87ce-6e895cc4020d\n","24/01/14 11:48:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-54f25242-9f9d-4c47-999a-976a11f7a1d6\n"]}]},{"cell_type":"code","source":["!cat salida301/*"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7yxdeiWKzcn_","executionInfo":{"status":"ok","timestamp":1705232992354,"user_tz":-60,"elapsed":383,"user":{"displayName":"Saioa Sada Allo","userId":"15281570099881323413"}},"outputId":"314f6420-89a8-4577-a9a6-3024827383ce"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["('Travel & Places', 4199507)\n"]}]}]}